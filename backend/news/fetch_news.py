# backend/app/news/fetch_news.py

import feedparser
from typing import List, Dict
from datetime import datetime, timedelta
from dateutil import parser as dateparser

# æ”¯æŒå¤šä¸ª RSS æº
RSS_FEEDS = {
    "BBC": "https://feeds.bbci.co.uk/news/rss.xml",
    "CNN": "https://rss.cnn.com/rss/edition.rss",
    "Reuters": "https://feeds.reuters.com/reuters/topNews",
    "NYTimes": "https://rss.nytimes.com/services/xml/rss/nyt/HomePage.xml",
    "The Guardian": "https://www.theguardian.com/world/rss",
    "Al Jazeera": "https://www.aljazeera.com/xml/rss/all.xml",
    "Bloomberg": "https://www.bloomberg.com/feed/podcast/etf-report.xml",
    "AP News": "https://apnews.com/rss/apf-topnews",
    "NPR": "https://feeds.npr.org/1001/rss.xml",
    "Financial Times": "https://www.ft.com/?format=rss",
    "Fox News": "https://feeds.foxnews.com/foxnews/latest",
    "Sky News": "https://feeds.skynews.com/feeds/rss/world.xml",
}

def get_tech_news() -> List[Dict]:
    items = []
    now = datetime.utcnow()
    one_day_ago = now - timedelta(days=1)

    for source_name, feed_url in RSS_FEEDS.items():
        try:
            feed = feedparser.parse(feed_url)
            for entry in feed.entries:
                # å°è¯•è§£ææ—¥æœŸ
                raw_date = getattr(entry, "published", "") or getattr(entry, "updated", "")
                try:
                    published_dt = dateparser.parse(raw_date)
                except Exception:
                    continue  # è·³è¿‡æ— æ³•è§£ææ—¥æœŸçš„æ¡ç›®

                # åªä¿ç•™ä¸€å¤©ä¹‹å†…çš„æ–°é—»
                if published_dt.tzinfo:
                    published_dt = published_dt.astimezone(tz=None).replace(tzinfo=None)
                if published_dt < one_day_ago:
                    continue

                # ä¼˜åŒ–å†…å®¹è·å–é€»è¾‘
                content = ""
                summary = ""
                
                # 1. ä¼˜å…ˆè·å–å®Œæ•´å†…å®¹
                if hasattr(entry, "content") and entry.content:
                    content = entry.content[0].value
                    print(f"ğŸ“° [DEBUG] {source_name} - Using content field, length: {len(content)}")
                # 2. å¦‚æœæ²¡æœ‰contentï¼Œå°è¯•è·å–summary
                elif hasattr(entry, "summary") and entry.summary:
                    content = entry.summary
                    print(f"ğŸ“° [DEBUG] {source_name} - Using summary as content, length: {len(content)}")
                # 3. å¦‚æœéƒ½æ²¡æœ‰ï¼Œè·³è¿‡è¿™æ¡æ–°é—»
                else:
                    print(f"âš ï¸ [WARNING] {source_name} - No content or summary found for: {entry.title}")
                    continue
                
                # ç¡®ä¿å†…å®¹ä¸ä¸ºç©º
                if not content or not str(content).strip():
                    print(f"âš ï¸ [WARNING] {source_name} - Empty content for: {entry.title}")
                    continue
                
                # åˆ›å»ºæ‘˜è¦ï¼ˆç”¨äºæ˜¾ç¤ºï¼‰
                summary = content[:600] if len(content) > 600 else content

                items.append({
                    "title": entry.title,
                    "content": content,  # å®Œæ•´å†…å®¹ç”¨äºAIæ‘˜è¦
                    "summary": summary,  # ç®€çŸ­æ‘˜è¦ç”¨äºæ˜¾ç¤º
                    "link": entry.link,
                    "date": raw_date,
                    "source": source_name
                })
                
                print(f"âœ… [DEBUG] {source_name} - Added article: {entry.title[:50]}... (content: {len(content)} chars)")
                
        except Exception as e:
            print(f"[error] Failed to fetch {source_name}: {e}")
    
    print(f"ğŸ“Š [DEBUG] Total articles fetched: {len(items)}")
    # æœ€ååŠ ä¸Šå…¨å±€æ’åº
    items.sort(key=lambda x: dateparser.parse(x["date"]), reverse=True)
    return items
