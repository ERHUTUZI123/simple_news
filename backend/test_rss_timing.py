#!/usr/bin/env python3
"""
æµ‹è¯•RSSæºçš„æ—¶é—´æ›´æ–°æƒ…å†µ
"""

import feedparser
from datetime import datetime, timedelta
from dateutil import parser as dateparser
from dateutil import tz
import logging

# è®¾ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# RSSæºé…ç½®
RSS_FEEDS = {
    # ç¾Žå›½ä¸»æµåª’ä½“
    "The New York Times": "https://rss.nytimes.com/services/xml/rss/nyt/HomePage.xml",
    "The Washington Post": "https://feeds.washingtonpost.com/rss/national",
    "Los Angeles Times": "https://www.latimes.com/local/rss2.0.xml",
    "NBC News": "https://feeds.nbcnews.com/nbcnews/public/world",
    "CBS News": "https://www.cbsnews.com/latest/rss/main",
    "ABC News": "https://feeds.abcnews.com/abcnews/usheadlines",
    "Fox News": "https://feeds.foxnews.com/foxnews/latest",
    "CNBC": "https://www.cnbc.com/id/100003114/device/rss/rss.html",
    "Axios": "https://api.axios.com/feed/",
    
    # å›½é™…æ–°é—»æœºæž„
    "Reuters": "https://feeds.reuters.com/reuters/topNews",
    "Associated Press": "https://apnews.com/rss/apf-topnews",
    "Bloomberg": "https://feeds.bloomberg.com/politics/news.rss",
    
    # è‹±å›½åª’ä½“
    "BBC News": "https://feeds.bbci.co.uk/news/rss.xml",
    "The Guardian": "https://www.theguardian.com/world/rss",
    "The Telegraph": "https://www.telegraph.co.uk/rss.xml",
    "Financial Times": "https://www.ft.com/?format=rss",
    "Sky News": "https://feeds.skynews.com/feeds/rss/world.xml",
    "The Independent": "https://www.independent.co.uk/news/world/rss",
    
    # æ¬§æ´²åª’ä½“
    "Euronews": "https://www.euronews.com/rss?format=mrss&level=theme&name=news",
    "Deutsche Welle": "https://rss.dw.com/xml/rss-de-all",
    
    # ä¸­ä¸œåª’ä½“
    "Al Jazeera": "https://www.aljazeera.com/xml/rss/all.xml",
}

def test_rss_timing():
    """æµ‹è¯•RSSæºçš„æ—¶é—´æ›´æ–°æƒ…å†µ"""
    now = datetime.utcnow()
    print(f"å½“å‰UTCæ—¶é—´: {now}")
    print(f"24å°æ—¶å‰: {now - timedelta(hours=24)}")
    print(f"6å°æ—¶å‰: {now - timedelta(hours=6)}")
    print("=" * 80)
    
    all_articles = []
    
    for source_name, feed_url in RSS_FEEDS.items():
        try:
            print(f"\nðŸ“° æ£€æŸ¥ {source_name}...")
            feed = feedparser.parse(feed_url)
            
            if not feed.entries:
                print(f"  âŒ æ²¡æœ‰æ‰¾åˆ°æ–‡ç« ")
                continue
                
            source_articles = []
            for i, entry in enumerate(feed.entries[:5]):  # åªæ£€æŸ¥å‰5ç¯‡æ–‡ç« 
                # å°è¯•è§£æžæ—¥æœŸ
                raw_date = getattr(entry, "published", "") or getattr(entry, "updated", "")
                
                if not raw_date:
                    print(f"  âš ï¸  æ–‡ç«  {i+1}: æ²¡æœ‰æ—¥æœŸä¿¡æ¯")
                    continue
                    
                try:
                    published_dt = dateparser.parse(raw_date)
                    
                    # æ­£ç¡®å¤„ç†æ—¶åŒºï¼šè½¬æ¢ä¸ºUTCè¿›è¡Œæ¯”è¾ƒ
                    if published_dt.tzinfo:
                        published_dt_utc = published_dt.astimezone(tz.tzutc())
                    else:
                        published_dt_utc = published_dt.replace(tzinfo=tz.tzutc())
                    
                    # è®¡ç®—æ—¶é—´å·®
                    time_diff = now - published_dt_utc.replace(tzinfo=None)
                    hours_ago = time_diff.total_seconds() / 3600
                    
                    article_info = {
                        "source": source_name,
                        "title": str(entry.title)[:50] + "..." if len(str(entry.title)) > 50 else str(entry.title),
                        "published": published_dt_utc,
                        "hours_ago": hours_ago,
                        "raw_date": raw_date
                    }
                    
                    source_articles.append(article_info)
                    
                    status = "âœ…" if hours_ago <= 24 else "âŒ"
                    print(f"  {status} æ–‡ç«  {i+1}: {hours_ago:.1f}å°æ—¶å‰ - {entry.title[:50]}...")
                    
                except Exception as e:
                    print(f"  âŒ æ–‡ç«  {i+1}: æ—¥æœŸè§£æžå¤±è´¥ - {raw_date} - {e}")
                    continue
            
            if source_articles:
                # æ‰¾åˆ°è¯¥æºæœ€æ–°çš„æ–‡ç« 
                latest_article = min(source_articles, key=lambda x: x["hours_ago"])
                all_articles.append(latest_article)
                print(f"  ðŸ“Š æœ€æ–°æ–‡ç« : {latest_article['hours_ago']:.1f}å°æ—¶å‰")
            else:
                print(f"  âŒ æ²¡æœ‰æœ‰æ•ˆæ–‡ç« ")
                
        except Exception as e:
            print(f"  âŒ èŽ·å–å¤±è´¥: {e}")
    
    print("\n" + "=" * 80)
    print("ðŸ“Š æ±‡æ€»æŠ¥å‘Š:")
    
    if all_articles:
        # æŒ‰æ—¶é—´æŽ’åº
        all_articles.sort(key=lambda x: x["hours_ago"])
        
        print(f"\næœ€æ–°æ–‡ç«  (å‰10):")
        for i, article in enumerate(all_articles[:10]):
            print(f"  {i+1}. {article['source']}: {article['hours_ago']:.1f}å°æ—¶å‰ - {article['title']}")
        
        # ç»Ÿè®¡ä¿¡æ¯
        recent_articles = [a for a in all_articles if a["hours_ago"] <= 6]
        recent_24h = [a for a in all_articles if a["hours_ago"] <= 24]
        
        print(f"\nðŸ“ˆ ç»Ÿè®¡:")
        print(f"  6å°æ—¶å†…: {len(recent_articles)}/{len(all_articles)} ä¸ªæº")
        print(f"  24å°æ—¶å†…: {len(recent_24h)}/{len(all_articles)} ä¸ªæº")
        print(f"  å¹³å‡æ›´æ–°æ—¶é—´: {sum(a['hours_ago'] for a in all_articles) / len(all_articles):.1f}å°æ—¶å‰")
        
        if recent_articles:
            print(f"  âœ… æœ‰ {len(recent_articles)} ä¸ªæºåœ¨6å°æ—¶å†…æ›´æ–°")
        else:
            print(f"  âš ï¸  æ²¡æœ‰æºåœ¨6å°æ—¶å†…æ›´æ–°ï¼Œå»ºè®®æ£€æŸ¥RSSæºæˆ–è°ƒæ•´æ—¶é—´è¿‡æ»¤")
    else:
        print("âŒ æ²¡æœ‰èŽ·å–åˆ°ä»»ä½•æœ‰æ•ˆæ–‡ç« ")

if __name__ == "__main__":
    test_rss_timing() 